# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JBfBothotOp5O5L6tHUiFJxUI8DTxtdL
"""

import torch
import random
import torch.nn as nn
import numpy as np
import torchvision
from torchvision import transforms
from torchvision.transforms import v2
from torchvision.transforms import ToTensor
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
device

!python --version

raw_train_data = datasets.CIFAR10(root="data",
                              train=True,
                              download=True,
                              transform=ToTensor(),
                              target_transform=None)

raw_test_data = datasets.CIFAR10(root="data",
                             train=False,
                             download=True,
                             transform=ToTensor(),
                             target_transform=None)

class_to_idx = raw_train_data.class_to_idx
class_names = raw_train_data.classes
class_names

def gaussian_blur(image: torch.Tensor) -> torch.Tensor:
    kernel_size = 5
    sigma = random.uniform(0.1, 2.0)

    gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)

    blurred_image = gaussian_blur(image)
    return blurred_image

def gaussian_noise(image: torch.Tensor) -> torch.Tensor:
    sigma = random.uniform(0.1, 0.5)
    gaussian_noise = transforms.v2.GaussianNoise(sigma=sigma)
    return gaussian_noise(image)

def jitter(image: torch.Tensor) -> torch.Tensor:

    jitter = transforms.ColorJitter(
        brightness=0.5,
        contrast=0.5,
        saturation=0.5,
        hue=0.3
    )

    jittered_image = jitter(image)

    return jittered_image

corruption_functions = [gaussian_blur, gaussian_noise, jitter]

train_data, test_data = [], []
train_labels, test_labels = [], []

for image, label in raw_train_data:
  train_data.append(image)
  train_labels.append(0)

  for corruption in corruption_functions:
    corrupted_image = corruption(image)
    train_data.append(corrupted_image)
    train_labels.append(1)

for image, label in raw_test_data:
  test_data.append(image)
  test_labels.append(0)

  for corruption in corruption_functions:
    corrupted_image = corruption(image)
    test_data.append(corrupted_image)
    test_labels.append(1)

class CorruptedCIFAR10(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        image = self.data[idx]
        label = self.labels[idx]
        return image, label

train_dataset = CorruptedCIFAR10(train_data, train_labels)
test_dataset = CorruptedCIFAR10(test_data, test_labels)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

len(train_dataset), len(test_dataset)

class myCorruptionModel(nn.Module):
    def __init__(self):
        super(myCorruptionModel, self).__init__()
        # Convolutional layers to capture spatial features
        self.conv_stack = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(negative_slope=0.01),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(negative_slope=0.01),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(negative_slope=0.01),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # New layer
            nn.BatchNorm2d(256),
            nn.LeakyReLU(negative_slope=0.01),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # Fully connected layers for classification
        self.fc_stack = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # Adaptive pooling
            nn.Flatten(),
            nn.Linear(256, 256),  # Update input size accordingly
            nn.ReLU(),
            nn.Dropout(p=0.5),  # Dropout for regularization
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv_stack(x)
        x = self.fc_stack(x)
        return x

torch.manual_seed(42)

model_0 = myCorruptionModel().to(device)

loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model_0.parameters(), lr=1e-4)

import requests
from pathlib import Path

# Download helper functions from Learn PyTorch repo (if not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  # Note: you need the "raw" GitHub URL for this to work
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)

from helper_functions import accuracy_fn
from timeit import default_timer as timer
def print_train_time(start: float, end: float, device: torch.device = None):
    """Prints difference between start and end time.

    Args:
        start (float): Start time of computation (preferred in timeit format).
        end (float): End time of computation.
        device ([type], optional): Device that compute is running on. Defaults to None.

    Returns:
        float: time between start and end in seconds (higher is longer).
    """
    total_time = end - start
    print(f"Train time on {device}: {total_time:.3f} seconds")
    return total_time

len(test_loader)

x, y = next(iter(train_loader))
x[0], y[0]

from tqdm.auto import tqdm
torch.manual_seed(42)

train_time_start_on_cpu = timer()

epochs = 100

for epoch in tqdm(range(epochs)):
  print(f"Epoch: {epoch}\n-----")

  train_loss = 0

  for batch, (X, y) in enumerate(train_loader):
    model_0.train()

    X, y = X.to(device), y.to(device)

    y_logits = model_0(X).to(device).squeeze()
    y_pred = torch.round(torch.sigmoid(y_logits))

    y = y.float()

    loss = loss_fn(y_logits, y)
    acc = accuracy_fn(y, y_pred)

    train_loss += loss

    optimizer.zero_grad()

    loss.backward()

    optimizer.step()

    if batch % 400 == 0:
      print(f"Looked at {batch * len(X)}/{len(train_loader.dataset)} samples")

  train_loss /= len(train_loader)
  test_loss, test_acc = 0, 0

  model_0.eval()
  with torch.inference_mode():
    for X_test, y_test in test_loader:
      X_test, y_test = X_test.to(device), y_test.to(device)

      test_logits = model_0(X_test).squeeze()
      test_pred = torch.round(torch.sigmoid(test_logits))

      y_test = y_test.float()

      test_loss += loss_fn(test_logits, y_test).item()
      test_acc += accuracy_fn(y_true=y_test,
                              y_pred=test_pred)

    test_loss /= len(test_loader)
    test_acc /= len(test_loader)

  print(f"\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\n")

train_time_end_on_cpu = timer()
total_train_time = print_train_time(start=train_time_start_on_cpu,
                                    end=train_time_end_on_cpu,
                                    device=str(next(model_0.parameters()).device))

image, label = next(iter(train_loader))
image = image[10]
label = label[10]

image, label = next(iter(train_loader))
image = image[25]
label = label[25]

image = image.unsqueeze(0).to(device)  # Add a batch dimension and move to device
print(image.shape)

model_0.eval()  # Set the model to evaluation mode
with torch.inference_mode():
    logits = model_0(image)  # Get the logits for the single image
    probabilities = torch.sigmoid(logits)  # Convert logits to probabilities
    prediction = torch.round(probabilities)  # Round to get the binary prediction

# Convert results to CPU for printing
logits = logits.cpu().item()
probabilities = probabilities.cpu().item()
prediction = prediction.cpu().item()

print(f'Label: {label}')
print(f'Logits: {logits:.4f}')
print(f'Probability: {probabilities:.4f}')
print(f'Prediction: {prediction}')

torch.save(model_0.state_dict(), 'model_0.pth')

